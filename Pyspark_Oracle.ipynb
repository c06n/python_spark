{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1532bbd9-8890-452a-81a7-47a2cce94ac8",
   "metadata": {},
   "source": [
    "# Oracle Spark Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e22e8-e8f5-42c0-9dc8-d9c651dd22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import os\n",
    "import random\n",
    "import cx_Oracle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be31e9-74c8-4cc5-9d13-e2c8df18f859",
   "metadata": {},
   "source": [
    "## Spark\n",
    "In the following you see Spark related configurations. First some Java Libraries are selected which should be uploaded to the spark cluster. These libraries come from this Jupyterlab instance and are baked into this Jupyter enviornment. The next command shows, which jar-files are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f599604-189d-46df-aa5e-6c25f7d7061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java libraries for Spark\n",
    "%system ls /opt/spark/jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566710d4-35e0-4a4e-854c-660bd4b3cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE_OPTIONS = '--packages %s ' % ','.join((\n",
    "#    'org.apache.hadoop:hadoop-aws:3.2.0',\n",
    "# ))\n",
    "\n",
    "JAR_OPTIONS = '--jars %s ' % ','.join((\n",
    "#    'file:///opt/spark/jars/postgresql-42.3.1.jar',\n",
    "    'file:///opt/spark/jars/ojdbc11.jar',\n",
    "))\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = JAR_OPTIONS + PACKAGE_OPTIONS + ' pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = JAR_OPTIONS + ' pyspark-shell'\n",
    "# os.environ['SPARK_EXTRA_CLASSPATH'] = '/opt/spark/jars/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3cbbb-85e0-4906-8544-93cc8b5c52e0",
   "metadata": {},
   "source": [
    "Next we define Spark configurations, which create a new Spark Cluster inside Kubernetes. You can leave the defaults unless you know what you are doing. You can control spark executors power by adjusting the `spark.executor` options. Also you can change the applicaiton name in `.setAppName()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bad7b-4e4b-408f-beed-35e37d609262",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = 'python3' # Needs to be explicitly provided as env. Otherwise workers run Python 2.7\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3'  # Same\n",
    "\n",
    "conf = (SparkConf().setMaster(\"k8s://https://kubernetes.default\")\n",
    "    .set(\"spark.kubernetes.container.image\", \"10.34.96.225/gemitec/spark-py:3.2.0-hadoop-3.2.0\")\n",
    "    .set(\"spark.driver.port\", \"2222\") # Needs to match svc\n",
    "    .set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "    .set(\"spark.driver.host\", \"driver-service.jupyterhub.svc.cluster.local\") # Needs to match svc\n",
    "    .set(\"spark.driver.bindAddress\", \"0.0.0.0\") #  Otherwise tries to bind to svc IP, will fail\n",
    "    .set(\"spark.kubernetes.namespace\", \"jupyterhub\")\n",
    "    .set(\"spark.jars\",\"file:///opt/spark/jars/ojdbc11.jar\") # Jars which should be uploaded\n",
    "     \n",
    "    # Set these to control the Spark computational power\n",
    "    .set(\"spark.executor.instances\", \"2\")\n",
    "    .set(\"spark.executor.memory\", \"2g\") # Right now do not enter more than 20g\n",
    "    .set(\"spark.executor.cores\", \"1\") # Right now do not enter more than 4\n",
    "    .setAppName('oracle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe3872-eaa0-429f-89ad-bdc37445b849",
   "metadata": {},
   "source": [
    "`SparkSession` starts the cluster. **Never forget** to use `spark.stop()` when you are finished. Otherwise the cluster will live forever and take up computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf507ea-e6c1-4524-9f49-5a852581c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94557667-5567-4212-9b83-3b60e0e7e02a",
   "metadata": {},
   "source": [
    "### Oracle\n",
    "We will connect to an oracle database. In this code cell we configure the connection options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d4bfa-adbd-46a5-bb7e-8e7499512cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"\"\n",
    "password = \"\"\n",
    "host = \"\"\n",
    "service = \"\"\n",
    "port = \n",
    "\n",
    "connection_str = f'{username}/{password}@{host}:{port}/{service}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eba526-9e8a-42be-a155-e642e6f98546",
   "metadata": {},
   "source": [
    "In this spark environment the oracle python client is installed. We can use it to test some things out before we connect spark with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82549673-2f3e-4a1f-b2af-c1819065640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = cx_Oracle.connect(connection_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec8de7-d9a7-489b-b2e9-c367ac41f94c",
   "metadata": {},
   "source": [
    "Here we just print the avialabe tables in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defa7a5-4b62-44aa-b7ea-957824718109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT table_name  FROM dba_tables\")\n",
    "for row in cursor:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1830627-d8a0-4a5c-9e91-4d1ea7620166",
   "metadata": {},
   "source": [
    "OK now that we know everything works. Let's use spark to connect to oracle and give us a table. We get back a dataframe-like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befadfcd-2673-4c30-a09e-842d623f36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_table_name = \"\"\n",
    "df_table = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:oracle:thin:{connection_str}\") \\\n",
    "    .option(\"dbtable\", db_table_name) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d718e-ccb6-44c6-b8a9-8381035a3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd912724-91e1-4b38-8302-7be4b452bd21",
   "metadata": {},
   "source": [
    "We can also perform SQL-queries with spark. This query gives you a dataframe back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60fa33-7080-425b-8271-07cdc4108086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_query = \"SELECT * FROM YOUR_TABLE_NAME;\"\n",
    "df_test = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:oracle:thin:{connection_str}\") \\\n",
    "    .option(\"dbtable\", sql_query) \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e750d9-d330-4f43-8947-448629105160",
   "metadata": {},
   "source": [
    "However you can also use the dataframe to perform queries on Spark: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ce792-58bc-42df-be2f-66982b07b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.select(expr('count(*)')).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b39036-73a6-4c48-a97a-64756892b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "We ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f9f2b-6eca-4a8b-8158-911d4a683857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.select(\"MOTOR_SPEED\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10c7d2-0b71-4c6b-b5d4-769d29730e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark .sql.functions import *\n",
    "dfm = df_table.select(((col('MOTOR_SPEED')) / (col('KILOMETRAGE')))*100)\n",
    "df = df_table.withColumn('dfm',(col('MOTOR_SPEED')/(col('KILOMETRAGE')) *100))\n",
    "df.take(10)\n",
    "# df.toPandas() # PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca925f-912b-4b32-bc63-5485291b338a",
   "metadata": {},
   "source": [
    "# Other examples\n",
    "Here we calculate the constant Pi using parallel processing in the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a10504-4833-47e4-a96c-53b2fd0ed259",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000000\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bce8b-0f0c-4d0b-9316-f0520fed9031",
   "metadata": {},
   "source": [
    "At the end of your calculations **ALWAYS** use this command to shutdown the sark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77622b34-3296-4f5c-a226-f2c91b26d23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
